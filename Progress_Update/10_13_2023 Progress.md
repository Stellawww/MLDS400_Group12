# October 13th Progress Update
## What we did
- Imported five tables into Jupyter Notebook for a preliminary check.
- Completed a first-round of cleaning but will require further data wrangling.

### deptinfo (Department Info)
- Two columns (`deptid`, `deptdesc`)
- 60 non-null rows
- There was no key insight to draw from this small table. It will be primarily used to join with `skuinfo` table.

### strinfo (Store Info)
- Four columns (`storeid`,`city`,`state`,`zip`)
- 453 non-null rows.
- Although there are 50 states in the United States, we discovered that the table only includes 31 unique states.
  - While we will use `storeid` to join tables, it is good to note that not all US states are available.

### skstinfo (SKU + Store Info)
- Four columns (`skuid`,`storeid`,`cost`,`retail`)
- Close to 2.5 million rows of data
- `cost`: Mean = $24.16, Stand Dev. = $40.57
- `retail`: Mean = $43.33, Stand Dev. = $83.92

### trans (Transaction)
- Nine Columns (`skuid`,`storeid`,`register`,`saledate`,`stype`,`quantity`,`orgprice`,`amt`)
- Excluded `seq`, `interid`, `mic` from the original table as they are unnecessary for further analysis.
- Around 681K rows of data.


## Our plan for next week
- Continue cleaning up the data and working on EDA.
- Attempt to import into SQL Server, if possible. (It does take a few minutes to rerun Jupyter Notebook, for we need to import and clean the GB-level CSV file again).
